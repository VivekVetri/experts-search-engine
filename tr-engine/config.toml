prefix = "."

dataset = "experts"
corpus = "line.toml"
index = "experts/idx"

stop-words = "experts/stopwords.txt"

query-judgements = "experts/experts-qrels.txt"

[[analyzers]]
method = "ngram-word"
ngram = 1
filter = [{type = "icu-tokenizer", suppress-tags = true}, {type = "lowercase"}]

#[embeddings]
#prefix = "word-embeddings"
#filter = [{type = "icu-tokenizer"}, {type = "lowercase"}]
#vector-size = 5
#[embeddings.vocab]
#min-count = 3
#max-size = 50

#[[analyzers]]
#method = "ngram-pos"
#ngram = 2
#filter = [{type = "icu-tokenizer"}, {type = "ptb-normalizer"}]
#crf-prefix = "experts/crf"
#

#[[analyzers]]
#method = "tree"
#filter = [{type = "icu-tokenizer"}, {type = "ptb-normalizer"}]
#features = ["subtree"]
#tagger = "experts/perceptron-tagger/"
#parser = "experts/parser"

#[ranker]
#method = "bm25"
#k1 = 1.2
#b = 0.75
#k3 = 500